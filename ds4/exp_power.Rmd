---
title: "causal inference"
author: gaurav sood
date: December 10, 2019
output:
  revealjs::revealjs_presentation:
    theme: simple
    incremental: true
    transition: slide
    highlight: haddock
    center: true
    fig_width: 5
    fig_height: 4
    css: reveal.css
    fig_caption: true
---

# cosal inference

## Casual Inference

* More frequent system crashes *cause* people to renew their software license.

* Five Habits of Successful People
  - Adopt these and you will be successful too!

* Mayor Giuliani brought the NYC crime rate down.

* I took this homeopathic medication and my headache went away.

## Sins

* Mistaking correlation for causation
  - No causation without manipulation

* Selecting on the dependent variable
  - Even correlation is not guaranteed!
  - Survivorship bias!

* Forgetting about ecological trends

* Treating temporally proximate as causal

* Not accounting for placebo effects

* In everyday conversation, people frequently make **four** other errors:
  - $x$ always causes $y$
  - $x$ causes $y$ to change a lot
  - $x$ is the **only** cause of $y$
  - the effect is the same across people

# Randomized Inference

## Limits of Observational Inference

* We can `control` for things but:
  - Omitted variables
  - Endogenous variables
  - Incorrect functional form
  - Heterogenous treatment effects

## How to infer consequences of manipulations?

* No causation without manipulation (Holland, 1986)
  - Otherwise why would the same people be exposed to different things?
  - Exploit luck or manufacture luck (intervene)
  - Prediction is predicting $Y$ after *observing* $X = x$.
  - Causation is predicting $Y$ after *setting* $X = x$.

## The Fundamental Problem of Causation

* | D    | $Y_0$ | $Y_1$ |
  |------|-------|-------|
  | 1    |   *   |  1    |
  | 1    |   *   |  0    |
  | 1    |   *   |  1    |
  | 0    |   1   |  *    |
  | 0    |   1   |  *    |
  | 0    |   0   |  *    |

## Comparing Groups

  - $E[Y_i| D_i = 1] - E[Y_i|D_i = 0]$
  - $E[Y_i(1)| D_i = 1] - E[Y_i(0)|D_i = 0]$
  - $E[Y_i(1)| D_i = 1] - E[Y_i(0)| D_i = 1] + E[Y_i(0)|D_i = 1] - E[Y_i(0)| D_i = 0]$
  - $E[Y_i(1) - Y_i(0)| D_i = 1] + E[Y_i(0)|D_i = 1] - E[Y_i(0)| D_i = 0]$
  - Naive = ATT + Selection Bias

## What Random Assignment Does

  - $P[D_i = 1]$ is probablity  of being assigned to treatment
  - Assignment doesn't depend on potential outcomes or is **unconfounded**
    - $P[D_i = 1|Y_i(0), Y_i(1)] = P[D_i = 1]$
    - SUTVA or No Interference
<br><br>
* Randomization nukes selection bias:
  - $E[Y_i| D_i = 1] - E[Y_i|D_i = 0]$
  - $E[Y_i(1) - Y_i(0)| D_i = 1] + E[Y_i(0)|D_i = 1] - E[Y_i(0)| D_i = 0]$
  - ATT + $E[Y_i(0)|D_i = 1] - E[Y_i(0)| D_i = 0]$
  - because of unconfoundedness
    - $E[Y_i(1)| D_i = 1] =  E[Y_i(0)| D_i = 0] = E[Y_i(1)] - E[Y_i(0)]$ 
<br><br>
* Leaves:
  - $E[Y_i(1)|D_i = 1] - E[Y_i(0)| D_i = 1]$
  - $E[Y_i(1)] - E[Y_i(0)]$

## Common Errors in Experiment Design

1. Optimistic Estimates of Effect Sizes

2. Bundling Treatments

3. Not Having a Strategy for Compliance

4. Interference
    - Learning
    - Self-selection

5. Implementation Failures
    - A/A tests
    - Expected n
    
6. Inadequate instrumentation
    - For CTR, we want: delivered, opened, clicked

7. Not measuring both the good and the bad

# Power

## Standard Error

* $s.e. = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$
* $s.e. \sim f(\hat{p}, n)$
* For a particular $n$, largest s.e. for $p = .5$
* If you want a s.e. no larger than $x$, how large should $n$ be for a particular $\alpha$ or a False Positive Rate?

* |                           | $H_0$ is true   | $H_0$ is false  |
  |-------------------------  |---------------  |---------------- |
  | **Reject $H_0$**          | FP ($\alpha$)   | Correct         |
  | **Do Not Reject $H_0$**   | Correct         | FN  ($\beta$)   |

* False Positive: $\alpha = .05$ and $1.96*s.e.$
* Power = $1 - \beta$ or the probability of not rejecting a null hypothesis when it is false.

## Standard Error of Diff. in Proportions

* Let $X$ and $Y$ be number of successes in group 1 and 2 respectively.
* Let $\hat{p} = \frac{X + Y}{n_1 + n_2}$
* $s.e. = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p} (1 - \hat{p}) (\frac{1}{n_1} + \frac{1}{n_2})}}$<br />
* Wald intervals don't work well.
* Agresti--Caffo intervals are better. <br />
  They use: $\tilde{n_1} = n_1 + 2, \tilde{p_1} = \frac{X + 1}{n_1 + 2}$ (same for $n_2$ and $p_2$)

## I Have the Power!

* Conventionally, studies seen as reasonably powered if $\beta >= .8$

* Let's think carefully about effect sizes.<br />
> If you send an email to encourage me to use a new product, <br />
  and measure success as proportion of people who follow-up on that, how many people do you think would follow-up?

* Here's how to think: Imagine a funnel:
    1. Assume that people must read the email for the email to have an effect.
    2. How many people will receive the email? Say **99\%.**
    3. How many people who receive will open the email? Say **5\%** of the **99\%.**
    4. How many people who open an email will read the email? Say **70\%** of the **5\%.**
    5. How many people who read an email will follow-up? Say **10\%.**
    6. Expected Number of **conversions** = $n*.99*.05*.70*.10$
    7. Expected effect size, compared to some low number = **.35\%**. We can make it **.5\%.**

## Power and Sizing

* Quantile Function: $Pr (X \leq x) = p$

* Rejection Region Using the Quantile Function

```
qbinom(p = 0.05, size = 100, prob = 0.3)
```

```
p1 = .05
p2 = .055
se = (.05)*(/sqrt(25)
a = 3.35 - 1.96 * se
b = 3.35 + 1.96 * se
c(a, b)
power = pnorm(a, 3.3, se) + (1 - pnorm(b, 3.3, se))
```

* $\beta = .8, \alpha = .05, \hat{p_1} = .005, \hat{p_2} = 0$
* $0 < .005 - 1.96*\frac{.005*.995}{\sqrt{n}}%
*

## Can We Do Better? --- Yes

1. Experiments Without Control <br />
   Give bananas to A and apples to B if you can assume that encouraging people to eat bananas doesn't cause them to eat more apples and vice-versa.

2. Optimal Stopping

# Opportunities and Issues

## Data Generating Process
  * SUTVA

## Decision Making:
  * Quantiles
    - [Netflix](https://medium.com/netflix-techblog/streaming-video-experimentation-at-netflix-visualizing-practical-and-statistical-significance-7117420f4e9a)
  * Bayesian
  * Trade-offs
  * Multiple Comparisons:
    - Bonferroni Correction (Boole's inequality): $P (T_i passes | H_0) \leq \frac{\alpha}{n}$
    - Assumes independence of outcomes
    - [WB](http://blogs.worldbank.org/impactevaluations/tools-of-the-trade-a-quick-adjustment-for-multiple-hypothesis-testing)

## Efficient Experimentation:
  * $k$-arm bandits
  * Optimal Stopping

## Observational Inference:
  * DID
  * IV
  * Double ML
