---
title: "error, bias, & variance"
author: gaurav sood
date: July 26, 2018
output:
  revealjs::revealjs_presentation:
    theme: simple
    incremental: true
    transition: slide
    highlight: haddock
    center: true
    fig_width: 5
    fig_height: 4
    css: reveal.css
    fig_caption: true
---


## Bias, Variance, and Error

* $Bias = E[\hat{f}(x) - f(x)]$

* $Variance = E[\hat{f}(x)^2] - (E[\hat{f}(x)])^2$

* $Error  = E[(y - \hat{f}(x))^2]$

* $E[(f(x)  + \epsilon - \hat{f}(x))^2]$

* $E[f(x) + \epsilon + E[\hat{f}(x)] - E[\hat{f}(x)] - \hat{f}(x))^2]$

* $(a + b + c)^2 = a^2 + b^2 + c^2 - 2ab - 2ac - 2bc$

* Expectation of sums is sum of expectations

* $E[(f(x) - E[\hat{f}(x)])^2] + E[\epsilon^2] + E[(E[\hat{f}(x)] - \hat{f}(x))^2] + \\
  2*E[(f(x) - E[\hat{f}(x)])]*E(\epsilon) + 2*E[(E[\hat{f}(x)] - \hat{f}(x))]*E[\epsilon] + \\
  2*E[(f(x) - E[\hat{f}(x)])]*E[(E[\hat{f}(x)] - \hat{f}(x))]$

## Continued ...

* $Bias^2 + Var(y) + Variance$

* $Bias^2 + \sigma^2 + Variance$

## 101 DL with Log Loss

* $z = w^{t}*x + b$

* $a = \sigma(z)$

* $y*log(a) - (1 - y)*log(1 - a)$

* Chain Rule: $\frac{\partial J}{\partial a} = \frac{\partial J}{\partial k} \frac{\partial k}{\partial a}$

* $\frac{\partial J}{\partial a} = -\frac{y}{a} \frac{1 - y}{1 - a}$

* $\frac{\partial a}{\partial z} = -\frac{y}{a} \frac{1 - y}{1 - a}$

